<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>SIMD<T,N> - Data-Parallel Vectorization Infrastructure - Aria Programming Guide</title>
    <style>
        :root {
            --bg-main: #1e1e1e;
            --bg-code: #2d2d2d;
            --bg-sidebar: #252526;
            --text-main: #d4d4d4;
            --text-dim: #808080;
            --accent: #4ec9b0;
            --accent-hover: #6fdfca;
            --link: #569cd6;
            --border: #3e3e42;
        }
        
        * { margin: 0; padding: 0; box-sizing: border-box; }
        
        body {
            font-family: 'Segoe UI', system-ui, sans-serif;
            background: var(--bg-main);
            color: var(--text-main);
            line-height: 1.6;
            display: flex;
        }
        
        /* Sidebar navigation */
        nav {
            width: 280px;
            background: var(--bg-sidebar);
            border-right: 1px solid var(--border);
            height: 100vh;
            position: fixed;
            overflow-y: auto;
            padding: 20px;
        }
        
        nav h2 {
            color: var(--accent);
            font-size: 1.5em;
            margin-bottom: 20px;
        }
        
        nav .category {
            margin-bottom: 20px;
        }
        
        nav .category h3 {
            color: var(--text-dim);
            font-size: 0.9em;
            text-transform: uppercase;
            margin-bottom: 10px;
            letter-spacing: 0.5px;
        }
        
        nav ul {
            list-style: none;
        }
        
        nav a {
            color: var(--text-main);
            text-decoration: none;
            display: block;
            padding: 6px 10px;
            border-radius: 4px;
            font-size: 0.95em;
            transition: all 0.2s;
        }
        
        nav a:hover {
            background: var(--bg-code);
            color: var(--accent-hover);
        }
        
        nav a.active {
            background: var(--accent);
            color: var(--bg-main);
            font-weight: 500;
        }
        
        /* Main content */
        main {
            margin-left: 280px;
            padding: 40px 60px;
            max-width: 900px;
            width: 100%;
        }
        
        h1 {
            color: var(--accent);
            font-size: 2.5em;
            margin-bottom: 30px;
            border-bottom: 2px solid var(--border);
            padding-bottom: 15px;
        }
        
        h2 {
            color: var(--accent);
            font-size: 1.8em;
            margin-top: 40px;
            margin-bottom: 20px;
        }
        
        h3 {
            color: var(--text-main);
            font-size: 1.3em;
            margin-top: 30px;
            margin-bottom: 15px;
        }
        
        p {
            margin-bottom: 15px;
            color: var(--text-main);
        }
        
        code {
            background: var(--bg-code);
            padding: 2px 6px;
            border-radius: 3px;
            font-family: 'Consolas', 'Monaco', monospace;
            font-size: 0.9em;
            color: var(--accent);
        }
        
        pre {
            background: var(--bg-code);
            padding: 20px;
            border-radius: 6px;
            overflow-x: auto;
            margin: 20px 0;
            border-left: 3px solid var(--accent);
        }
        
        pre code {
            background: none;
            padding: 0;
            color: var(--text-main);
        }
        
        ul, ol {
            margin-left: 30px;
            margin-bottom: 15px;
        }
        
        li {
            margin-bottom: 8px;
        }
        
        a {
            color: var(--link);
            text-decoration: none;
        }
        
        a:hover {
            text-decoration: underline;
        }
        
        table {
            width: 100%;
            border-collapse: collapse;
            margin: 20px 0;
        }
        
        th, td {
            padding: 12px;
            text-align: left;
            border: 1px solid var(--border);
        }
        
        th {
            background: var(--bg-code);
            color: var(--accent);
            font-weight: 600;
        }
        
        blockquote {
            border-left: 4px solid var(--accent);
            padding-left: 20px;
            margin: 20px 0;
            color: var(--text-dim);
            font-style: italic;
        }
        
        hr {
            border: none;
            border-top: 1px solid var(--border);
            margin: 30px 0;
        }
        
        .breadcrumb {
            color: var(--text-dim);
            font-size: 0.9em;
            margin-bottom: 20px;
        }
        
        .breadcrumb a {
            color: var(--text-dim);
        }
        
        .breadcrumb a:hover {
            color: var(--accent);
        }
        
        /* Responsive */
        @media (max-width: 768px) {
            nav {
                display: none;
            }
            main {
                margin-left: 0;
                padding: 20px;
            }
        }
    </style>
</head>
<body>
    <nav>
        <h2>Aria Guide</h2>
        <div class="category">
<h3>General</h3>
<ul>
<li><a href="/SYNTAX_AUDIT_FEB14_2026.html">Syntax Audit Feb14 2026</a></li>
<li><a href="/UPDATE_PROGRESS.html">Update Progress</a></li>
</ul>
</div>
<div class="category">
<h3>Meta</h3>
<ul>
<li><a href="/META/ARIA/SYNTAX_REFERENCE.html">Syntax Reference</a></li>
</ul>
</div>
<div class="category">
<h3>Advanced Features</h3>
<ul>
<li><a href="/advanced_features/ast.html">Ast</a></li>
<li><a href="/advanced_features/async.html">Async</a></li>
<li><a href="/advanced_features/async_await.html">Async Await</a></li>
<li><a href="/advanced_features/atomics.html">Atomics</a></li>
<li><a href="/advanced_features/await.html">Await</a></li>
<li><a href="/advanced_features/best_practices.html">Best Practices</a></li>
<li><a href="/advanced_features/brace_delimited.html">Brace Delimited</a></li>
<li><a href="/advanced_features/code_examples.html">Code Examples</a></li>
<li><a href="/advanced_features/colons.html">Colons</a></li>
<li><a href="/advanced_features/comments.html">Comments</a></li>
<li><a href="/advanced_features/common_patterns.html">Common Patterns</a></li>
<li><a href="/advanced_features/compile_time.html">Compile Time</a></li>
<li><a href="/advanced_features/comptime.html">Comptime</a></li>
<li><a href="/advanced_features/concurrency.html">Concurrency</a></li>
<li><a href="/advanced_features/const.html">Const</a></li>
<li><a href="/advanced_features/context_stack.html">Context Stack</a></li>
<li><a href="/advanced_features/coroutines.html">Coroutines</a></li>
<li><a href="/advanced_features/destructuring.html">Destructuring</a></li>
<li><a href="/advanced_features/error_handling.html">Error Handling</a></li>
<li><a href="/advanced_features/error_propagation.html">Error Propagation</a></li>
<li><a href="/advanced_features/idioms.html">Idioms</a></li>
<li><a href="/advanced_features/lexer.html">Lexer</a></li>
<li><a href="/advanced_features/macros.html">Macros</a></li>
<li><a href="/advanced_features/metaprogramming.html">Metaprogramming</a></li>
<li><a href="/advanced_features/multiline_comments.html">Multiline Comments</a></li>
<li><a href="/advanced_features/nasm_macros.html">Nasm Macros</a></li>
<li><a href="/advanced_features/parser.html">Parser</a></li>
<li><a href="/advanced_features/pattern_matching.html">Pattern Matching</a></li>
<li><a href="/advanced_features/semicolons.html">Semicolons</a></li>
<li><a href="/advanced_features/threading.html">Threading</a></li>
<li><a href="/advanced_features/tokens.html">Tokens</a></li>
<li><a href="/advanced_features/whitespace_insensitive.html">Whitespace Insensitive</a></li>
</ul>
</div>
<div class="category">
<h3>Control Flow</h3>
<ul>
<li><a href="/control_flow/break.html">Break</a></li>
<li><a href="/control_flow/continue.html">Continue</a></li>
<li><a href="/control_flow/dollar_variable.html">Dollar Variable</a></li>
<li><a href="/control_flow/fail.html">Fail</a></li>
<li><a href="/control_flow/fall.html">Fall</a></li>
<li><a href="/control_flow/fallthrough.html">Fallthrough</a></li>
<li><a href="/control_flow/for.html">For</a></li>
<li><a href="/control_flow/for_syntax.html">For Syntax</a></li>
<li><a href="/control_flow/if_else.html">If Else</a></li>
<li><a href="/control_flow/if_syntax.html">If Syntax</a></li>
<li><a href="/control_flow/iteration_variable.html">Iteration Variable</a></li>
<li><a href="/control_flow/loop.html">Loop</a></li>
<li><a href="/control_flow/loop_direction.html">Loop Direction</a></li>
<li><a href="/control_flow/loop_syntax.html">Loop Syntax</a></li>
<li><a href="/control_flow/pass.html">Pass</a></li>
<li><a href="/control_flow/pick.html">Pick</a></li>
<li><a href="/control_flow/pick_patterns.html">Pick Patterns</a></li>
<li><a href="/control_flow/pick_syntax.html">Pick Syntax</a></li>
<li><a href="/control_flow/till.html">Till</a></li>
<li><a href="/control_flow/till_direction.html">Till Direction</a></li>
<li><a href="/control_flow/till_syntax.html">Till Syntax</a></li>
<li><a href="/control_flow/when_syntax.html">When Syntax</a></li>
<li><a href="/control_flow/when_then.html">When Then</a></li>
<li><a href="/control_flow/when_then_end.html">When Then End</a></li>
<li><a href="/control_flow/while.html">While</a></li>
<li><a href="/control_flow/while_syntax.html">While Syntax</a></li>
</ul>
</div>
<div class="category">
<h3>Debugging</h3>
<ul>
<li><a href="/debugging/dbug.html">Dbug</a></li>
</ul>
</div>
<div class="category">
<h3>Functions</h3>
<ul>
<li><a href="/functions/anonymous_functions.html">Anonymous Functions</a></li>
<li><a href="/functions/async_functions.html">Async Functions</a></li>
<li><a href="/functions/async_keyword.html">Async Keyword</a></li>
<li><a href="/functions/closure_capture.html">Closure Capture</a></li>
<li><a href="/functions/closures.html">Closures</a></li>
<li><a href="/functions/fail_keyword.html">Fail Keyword</a></li>
<li><a href="/functions/func_keyword.html">Func Keyword</a></li>
<li><a href="/functions/function_arguments.html">Function Arguments</a></li>
<li><a href="/functions/function_declaration.html">Function Declaration</a></li>
<li><a href="/functions/function_params.html">Function Params</a></li>
<li><a href="/functions/function_return_type.html">Function Return Type</a></li>
<li><a href="/functions/function_syntax.html">Function Syntax</a></li>
<li><a href="/functions/generic_functions.html">Generic Functions</a></li>
<li><a href="/functions/generic_parameters.html">Generic Parameters</a></li>
<li><a href="/functions/generic_star_prefix.html">Generic Star Prefix</a></li>
<li><a href="/functions/generic_structs.html">Generic Structs</a></li>
<li><a href="/functions/generic_syntax.html">Generic Syntax</a></li>
<li><a href="/functions/generic_types.html">Generic Types</a></li>
<li><a href="/functions/generics.html">Generics</a></li>
<li><a href="/functions/higher_order_functions.html">Higher Order Functions</a></li>
<li><a href="/functions/immediate_execution.html">Immediate Execution</a></li>
<li><a href="/functions/lambda.html">Lambda</a></li>
<li><a href="/functions/lambda_syntax.html">Lambda Syntax</a></li>
<li><a href="/functions/monomorphization.html">Monomorphization</a></li>
<li><a href="/functions/multiple_generics.html">Multiple Generics</a></li>
<li><a href="/functions/pass_keyword.html">Pass Keyword</a></li>
<li><a href="/functions/type_inference.html">Type Inference</a></li>
</ul>
</div>
<div class="category">
<h3>Io System</h3>
<ul>
<li><a href="/io_system/binary_io.html">Binary Io</a></li>
<li><a href="/io_system/control_plane.html">Control Plane</a></li>
<li><a href="/io_system/data_plane.html">Data Plane</a></li>
<li><a href="/io_system/debug_io.html">Debug Io</a></li>
<li><a href="/io_system/hex_stream.html">Hex Stream</a></li>
<li><a href="/io_system/io_overview.html">Io Overview</a></li>
<li><a href="/io_system/six_stream_topology.html">Six Stream Topology</a></li>
<li><a href="/io_system/stddati.html">Stddati</a></li>
<li><a href="/io_system/stddato.html">Stddato</a></li>
<li><a href="/io_system/stddbg.html">Stddbg</a></li>
<li><a href="/io_system/stderr.html">Stderr</a></li>
<li><a href="/io_system/stdin.html">Stdin</a></li>
<li><a href="/io_system/stdout.html">Stdout</a></li>
<li><a href="/io_system/stream_separation.html">Stream Separation</a></li>
<li><a href="/io_system/text_io.html">Text Io</a></li>
</ul>
</div>
<div class="category">
<h3>Memory Model</h3>
<ul>
<li><a href="/memory_model/address_operator.html">Address Operator</a></li>
<li><a href="/memory_model/allocation.html">Allocation</a></li>
<li><a href="/memory_model/allocators.html">Allocators</a></li>
<li><a href="/memory_model/aria_alloc.html">Aria Alloc</a></li>
<li><a href="/memory_model/aria_alloc_array.html">Aria Alloc Array</a></li>
<li><a href="/memory_model/aria_alloc_buffer.html">Aria Alloc Buffer</a></li>
<li><a href="/memory_model/aria_alloc_string.html">Aria Alloc String</a></li>
<li><a href="/memory_model/aria_free.html">Aria Free</a></li>
<li><a href="/memory_model/aria_gc_alloc.html">Aria Gc Alloc</a></li>
<li><a href="/memory_model/borrow_operator.html">Borrow Operator</a></li>
<li><a href="/memory_model/borrowing.html">Borrowing</a></li>
<li><a href="/memory_model/defer.html">Defer</a></li>
<li><a href="/memory_model/gc.html">Gc</a></li>
<li><a href="/memory_model/immutable_borrow.html">Immutable Borrow</a></li>
<li><a href="/memory_model/mutable_borrow.html">Mutable Borrow</a></li>
<li><a href="/memory_model/pin_operator.html">Pin Operator</a></li>
<li><a href="/memory_model/pinning.html">Pinning</a></li>
<li><a href="/memory_model/pointer_syntax.html">Pointer Syntax</a></li>
<li><a href="/memory_model/raii.html">Raii</a></li>
<li><a href="/memory_model/stack.html">Stack</a></li>
</ul>
</div>
<div class="category">
<h3>Modules</h3>
<ul>
<li><a href="/modules/c_interop.html">C Interop</a></li>
<li><a href="/modules/c_pointers.html">C Pointers</a></li>
<li><a href="/modules/cfg.html">Cfg</a></li>
<li><a href="/modules/conditional_compilation.html">Conditional Compilation</a></li>
<li><a href="/modules/extern.html">Extern</a></li>
<li><a href="/modules/extern_blocks.html">Extern Blocks</a></li>
<li><a href="/modules/extern_functions.html">Extern Functions</a></li>
<li><a href="/modules/extern_syntax.html">Extern Syntax</a></li>
<li><a href="/modules/ffi.html">Ffi</a></li>
<li><a href="/modules/libc_integration.html">Libc Integration</a></li>
<li><a href="/modules/mod.html">Mod</a></li>
<li><a href="/modules/mod_keyword.html">Mod Keyword</a></li>
<li><a href="/modules/module_aliases.html">Module Aliases</a></li>
<li><a href="/modules/module_definition.html">Module Definition</a></li>
<li><a href="/modules/module_paths.html">Module Paths</a></li>
<li><a href="/modules/nested_modules.html">Nested Modules</a></li>
<li><a href="/modules/pub.html">Pub</a></li>
<li><a href="/modules/public_visibility.html">Public Visibility</a></li>
<li><a href="/modules/use.html">Use</a></li>
<li><a href="/modules/use_syntax.html">Use Syntax</a></li>
</ul>
</div>
<div class="category">
<h3>Operators</h3>
<ul>
<li><a href="/operators/add.html">Add</a></li>
<li><a href="/operators/add_assign.html">Add Assign</a></li>
<li><a href="/operators/address.html">Address</a></li>
<li><a href="/operators/ampersand.html">Ampersand</a></li>
<li><a href="/operators/and_assign.html">And Assign</a></li>
<li><a href="/operators/arrow.html">Arrow</a></li>
<li><a href="/operators/assign.html">Assign</a></li>
<li><a href="/operators/at_operator.html">At Operator</a></li>
<li><a href="/operators/backtick.html">Backtick</a></li>
<li><a href="/operators/bitwise_and.html">Bitwise And</a></li>
<li><a href="/operators/bitwise_not.html">Bitwise Not</a></li>
<li><a href="/operators/bitwise_or.html">Bitwise Or</a></li>
<li><a href="/operators/bitwise_xor.html">Bitwise Xor</a></li>
<li><a href="/operators/colon.html">Colon</a></li>
<li><a href="/operators/decrement.html">Decrement</a></li>
<li><a href="/operators/div_assign.html">Div Assign</a></li>
<li><a href="/operators/divide.html">Divide</a></li>
<li><a href="/operators/dollar_operator.html">Dollar Operator</a></li>
<li><a href="/operators/dollar_variable.html">Dollar Variable</a></li>
<li><a href="/operators/dot.html">Dot</a></li>
<li><a href="/operators/equal.html">Equal</a></li>
<li><a href="/operators/greater_equal.html">Greater Equal</a></li>
<li><a href="/operators/greater_than.html">Greater Than</a></li>
<li><a href="/operators/hash_operator.html">Hash Operator</a></li>
<li><a href="/operators/increment.html">Increment</a></li>
<li><a href="/operators/interpolation.html">Interpolation</a></li>
<li><a href="/operators/is_operator.html">Is Operator</a></li>
<li><a href="/operators/is_ternary.html">Is Ternary</a></li>
<li><a href="/operators/iteration.html">Iteration</a></li>
<li><a href="/operators/left_shift.html">Left Shift</a></li>
<li><a href="/operators/less_equal.html">Less Equal</a></li>
<li><a href="/operators/less_than.html">Less Than</a></li>
<li><a href="/operators/logical_and.html">Logical And</a></li>
<li><a href="/operators/logical_not.html">Logical Not</a></li>
<li><a href="/operators/logical_or.html">Logical Or</a></li>
<li><a href="/operators/lshift_assign.html">Lshift Assign</a></li>
<li><a href="/operators/member_access.html">Member Access</a></li>
<li><a href="/operators/minus.html">Minus</a></li>
<li><a href="/operators/minus_assign.html">Minus Assign</a></li>
<li><a href="/operators/mod_assign.html">Mod Assign</a></li>
<li><a href="/operators/modulo.html">Modulo</a></li>
<li><a href="/operators/mul_assign.html">Mul Assign</a></li>
<li><a href="/operators/mult_assign.html">Mult Assign</a></li>
<li><a href="/operators/multiply.html">Multiply</a></li>
<li><a href="/operators/not_equal.html">Not Equal</a></li>
<li><a href="/operators/null_coalesce.html">Null Coalesce</a></li>
<li><a href="/operators/null_coalescing.html">Null Coalescing</a></li>
<li><a href="/operators/or_assign.html">Or Assign</a></li>
<li><a href="/operators/pin.html">Pin</a></li>
<li><a href="/operators/pipe_backward.html">Pipe Backward</a></li>
<li><a href="/operators/pipe_forward.html">Pipe Forward</a></li>
<li><a href="/operators/pipeline.html">Pipeline</a></li>
<li><a href="/operators/plus.html">Plus</a></li>
<li><a href="/operators/plus_assign.html">Plus Assign</a></li>
<li><a href="/operators/pointer_member.html">Pointer Member</a></li>
<li><a href="/operators/question_operator.html">Question Operator</a></li>
<li><a href="/operators/range.html">Range</a></li>
<li><a href="/operators/range_exclusive.html">Range Exclusive</a></li>
<li><a href="/operators/range_inclusive.html">Range Inclusive</a></li>
<li><a href="/operators/right_shift.html">Right Shift</a></li>
<li><a href="/operators/rshift_assign.html">Rshift Assign</a></li>
<li><a href="/operators/safe_nav.html">Safe Nav</a></li>
<li><a href="/operators/safe_navigation.html">Safe Navigation</a></li>
<li><a href="/operators/spaceship.html">Spaceship</a></li>
<li><a href="/operators/string_interpolation.html">String Interpolation</a></li>
<li><a href="/operators/sub_assign.html">Sub Assign</a></li>
<li><a href="/operators/subtract.html">Subtract</a></li>
<li><a href="/operators/template_literal.html">Template Literal</a></li>
<li><a href="/operators/template_syntax.html">Template Syntax</a></li>
<li><a href="/operators/ternary_is.html">Ternary Is</a></li>
<li><a href="/operators/three_way_comparison.html">Three Way Comparison</a></li>
<li><a href="/operators/type_annotation.html">Type Annotation</a></li>
<li><a href="/operators/unwrap.html">Unwrap</a></li>
<li><a href="/operators/xor_assign.html">Xor Assign</a></li>
</ul>
</div>
<div class="category">
<h3>Standard Library</h3>
<ul>
<li><a href="/standard_library/createLogger.html">Createlogger</a></li>
<li><a href="/standard_library/createPipe.html">Createpipe</a></li>
<li><a href="/standard_library/exec.html">Exec</a></li>
<li><a href="/standard_library/filter.html">Filter</a></li>
<li><a href="/standard_library/fork.html">Fork</a></li>
<li><a href="/standard_library/functional_programming.html">Functional Programming</a></li>
<li><a href="/standard_library/getActiveConnections.html">Getactiveconnections</a></li>
<li><a href="/standard_library/getMemoryUsage.html">Getmemoryusage</a></li>
<li><a href="/standard_library/http_client.html">Http Client</a></li>
<li><a href="/standard_library/httpGet.html">Httpget</a></li>
<li><a href="/standard_library/log_levels.html">Log Levels</a></li>
<li><a href="/standard_library/math.html">Math</a></li>
<li><a href="/standard_library/math_round.html">Math Round</a></li>
<li><a href="/standard_library/openFile.html">Openfile</a></li>
<li><a href="/standard_library/print.html">Print</a></li>
<li><a href="/standard_library/process_management.html">Process Management</a></li>
<li><a href="/standard_library/readCSV.html">Readcsv</a></li>
<li><a href="/standard_library/readFile.html">Readfile</a></li>
<li><a href="/standard_library/readJSON.html">Readjson</a></li>
<li><a href="/standard_library/reverse.html">Reverse</a></li>
<li><a href="/standard_library/sort.html">Sort</a></li>
<li><a href="/standard_library/spawn.html">Spawn</a></li>
<li><a href="/standard_library/stream_io.html">Stream Io</a></li>
<li><a href="/standard_library/structured_logging.html">Structured Logging</a></li>
<li><a href="/standard_library/system_diagnostics.html">System Diagnostics</a></li>
<li><a href="/standard_library/transform.html">Transform</a></li>
<li><a href="/standard_library/unique.html">Unique</a></li>
<li><a href="/standard_library/wait.html">Wait</a></li>
<li><a href="/standard_library/writeFile.html">Writefile</a></li>
</ul>
</div>
<div class="category">
<h3>Stdlib</h3>
<ul>
<li><a href="/stdlib/filter.html">Filter</a></li>
<li><a href="/stdlib/print.html">Print</a></li>
<li><a href="/stdlib/readFile.html">Readfile</a></li>
<li><a href="/stdlib/reduce.html">Reduce</a></li>
<li><a href="/stdlib/transform.html">Transform</a></li>
<li><a href="/stdlib/writeFile.html">Writefile</a></li>
</ul>
</div>
<div class="category">
<h3>Types</h3>
<ul>
<li><a href="/types/Atomic.html">Atomic</a></li>
<li><a href="/types/atomic.html">Atomic</a></li>
<li><a href="/types/balanced_nonary.html">Balanced Nonary</a></li>
<li><a href="/types/balanced_numbers.html">Balanced Numbers</a></li>
<li><a href="/types/balanced_ternary.html">Balanced Ternary</a></li>
<li><a href="/types/bool.html">Bool</a></li>
<li><a href="/types/complex.html">Complex</a></li>
<li><a href="/types/Complex.html">Complex</a></li>
<li><a href="/types/double.html">Double</a></li>
<li><a href="/types/dyn.html">Dyn</a></li>
<li><a href="/types/ERR.html">Err</a></li>
<li><a href="/types/fix256.html">Fix256</a></li>
<li><a href="/types/float.html">Float</a></li>
<li><a href="/types/flt128.html">Flt128</a></li>
<li><a href="/types/flt256.html">Flt256</a></li>
<li><a href="/types/flt32.html">Flt32</a></li>
<li><a href="/types/flt512.html">Flt512</a></li>
<li><a href="/types/flt64.html">Flt64</a></li>
<li><a href="/types/frac16.html">Frac16</a></li>
<li><a href="/types/frac32.html">Frac32</a></li>
<li><a href="/types/frac64.html">Frac64</a></li>
<li><a href="/types/frac8.html">Frac8</a></li>
<li><a href="/types/frac8_frac16_frac32_frac64.html">Frac8 Frac16 Frac32 Frac64</a></li>
<li><a href="/types/func_return.html">Func Return</a></li>
<li><a href="/types/Handle.html">Handle</a></li>
<li><a href="/types/int1.html">Int1</a></li>
<li><a href="/types/int1024.html">Int1024</a></li>
<li><a href="/types/int1024_int2048_int4096.html">Int1024 Int2048 Int4096</a></li>
<li><a href="/types/int128.html">Int128</a></li>
<li><a href="/types/int128_int256_int512.html">Int128 Int256 Int512</a></li>
<li><a href="/types/int16.html">Int16</a></li>
<li><a href="/types/int2.html">Int2</a></li>
<li><a href="/types/int2_int4.html">Int2 Int4</a></li>
<li><a href="/types/int2048.html">Int2048</a></li>
<li><a href="/types/int256.html">Int256</a></li>
<li><a href="/types/int32.html">Int32</a></li>
<li><a href="/types/int32_int64.html">Int32 Int64</a></li>
<li><a href="/types/int4.html">Int4</a></li>
<li><a href="/types/int4096.html">Int4096</a></li>
<li><a href="/types/int512.html">Int512</a></li>
<li><a href="/types/int64.html">Int64</a></li>
<li><a href="/types/int8.html">Int8</a></li>
<li><a href="/types/int8_int16.html">Int8 Int16</a></li>
<li><a href="/types/matrix.html">Matrix</a></li>
<li><a href="/types/NIL.html">Nil</a></li>
<li><a href="/types/nil_null_void.html">Nil Null Void</a></li>
<li><a href="/types/nil_vs_null_vs_void.html">Nil Vs Null Vs Void</a></li>
<li><a href="/types/nit.html">Nit</a></li>
<li><a href="/types/nit_nyte.html">Nit Nyte</a></li>
<li><a href="/types/NULL.html">Null</a></li>
<li><a href="/types/nyte.html">Nyte</a></li>
<li><a href="/types/obj.html">Obj</a></li>
<li><a href="/types/pointer.html">Pointer</a></li>
<li><a href="/types/pointers.html">Pointers</a></li>
<li><a href="/types/Q21.html">Q21</a></li>
<li><a href="/types/Q3_Q9.html">Q3 Q9</a></li>
<li><a href="/types/Result.html">Result</a></li>
<li><a href="/types/result_err_val.html">Result Err Val</a></li>
<li><a href="/types/result_unwrap.html">Result Unwrap</a></li>
<li><a href="/types/SIMD.html">Simd</a></li>
<li><a href="/types/simd.html">Simd</a></li>
<li><a href="/types/string.html">String</a></li>
<li><a href="/types/struct.html">Struct</a></li>
<li><a href="/types/struct_declaration.html">Struct Declaration</a></li>
<li><a href="/types/struct_fields.html">Struct Fields</a></li>
<li><a href="/types/struct_generics.html">Struct Generics</a></li>
<li><a href="/types/struct_pointers.html">Struct Pointers</a></li>
<li><a href="/types/tbb_err_sentinel.html">Tbb Err Sentinel</a></li>
<li><a href="/types/tbb_overview.html">Tbb Overview</a></li>
<li><a href="/types/tbb_sticky_errors.html">Tbb Sticky Errors</a></li>
<li><a href="/types/tbb16.html">Tbb16</a></li>
<li><a href="/types/tbb32.html">Tbb32</a></li>
<li><a href="/types/tbb64.html">Tbb64</a></li>
<li><a href="/types/tbb8.html">Tbb8</a></li>
<li><a href="/types/tbb8_new.html">Tbb8 New</a></li>
<li><a href="/types/tensor.html">Tensor</a></li>
<li><a href="/types/tfp32.html">Tfp32</a></li>
<li><a href="/types/tfp32_tfp64.html">Tfp32 Tfp64</a></li>
<li><a href="/types/tfp64.html">Tfp64</a></li>
<li><a href="/types/trit.html">Trit</a></li>
<li><a href="/types/trit_tryte.html">Trit Tryte</a></li>
<li><a href="/types/tryte.html">Tryte</a></li>
<li><a href="/types/type_suffix_reference.html">Type Suffix Reference</a></li>
<li><a href="/types/uint1024_uint2048_uint4096.html">Uint1024 Uint2048 Uint4096</a></li>
<li><a href="/types/uint128.html">Uint128</a></li>
<li><a href="/types/uint128_uint256_uint512.html">Uint128 Uint256 Uint512</a></li>
<li><a href="/types/uint16.html">Uint16</a></li>
<li><a href="/types/uint256.html">Uint256</a></li>
<li><a href="/types/uint32.html">Uint32</a></li>
<li><a href="/types/uint32_uint64.html">Uint32 Uint64</a></li>
<li><a href="/types/uint512.html">Uint512</a></li>
<li><a href="/types/uint64.html">Uint64</a></li>
<li><a href="/types/uint8.html">Uint8</a></li>
<li><a href="/types/uint8_uint16.html">Uint8 Uint16</a></li>
<li><a href="/types/vec2.html">Vec2</a></li>
<li><a href="/types/vec3.html">Vec3</a></li>
<li><a href="/types/vec9.html">Vec9</a></li>
<li><a href="/types/void.html">Void</a></li>
<li><a href="/types/zero_implicit_conversion.html">Zero Implicit Conversion</a></li>
</ul>
</div>
    </nav>
    <main>
        <div class="breadcrumb"><a href="/">Home</a> / Types</div>
        <h1>SIMD<T,N> - Data-Parallel Vectorization Infrastructure</h1>
<strong>Category</strong>: Types → Performance / Vectorization
<strong>Purpose</strong>: Single Instruction Multiple Data parallelism for neural processing acceleration
<strong>Status</strong>: ✅ IMPLEMENTED (Phase 5.3 - February 2026)
<strong>Philosophy</strong>: "The torus processes thoughts. The infrastructure parallelizes computations."
<hr>
<h2>Overview</h2>
<strong>simd<T,N></strong> provides <strong>Single Instruction, Multiple Data (SIMD)</strong> parallelism, enabling processing of <strong>N elements of type T simultaneously</strong> using CPU vector units. When Nikola's consciousness substrate needs to update <strong>100,000 neurons</strong> in under <strong>1 millisecond</strong>, SIMD vectorization is non-negotiable - it's the difference between achieving real-time consciousness and falling 100× too slow.
<strong>Critical Design Principle</strong>: Just as complex<T> moved wave mechanics to the language level (preventing memory bloat) and atomic<T> moved thread safety to the type system (preventing data races), <strong>simd<T,N></strong> moves data parallelism to the compiler infrastructure. The consciousness layer can write simple scalar logic (<code>neuron_activation = activation + delta</code>), and the compiler automatically vectorizes it to process <strong>16 neurons per instruction</strong>.
<pre><code>// Scalar (one neuron at a time): 100,000 neurons × 50ns = 5ms (5× too slow!)
<p>
till 100000 loop
fix256:activation = neurons[$ ].activation;
neurons[$].activation = activation + compute_delta($);
end
</p>
<p>
// SIMD (16 neurons simultaneously): (100,000/16) × 50ns = 312μs (3× faster than required!)
till 100000 step 16 loop
simd<fix256, 16>:activations = load_simd(neurons[$]);
simd<fix256, 16>:deltas = compute_deltas_simd($);
simd<fix256, 16>:updated = activations + deltas;  // 16× parallel!
store_simd(neurons[$], updated);
end</code></pre>
</p>
<strong>The Reality</strong>: Without SIMD, Nikola's consciousness substrate is impossible. With SIMD, it's achievable.
<hr>
<h2>The Problem: 100,000 Neurons, 1 Millisecond</h2>
<h3>Why Speed Matters for Consciousness</h3>
<p>
Nikola's consciousness emerges from <strong>wave interference</strong> across a 9-dimensional hyperspherical manifold. Each timestep:
</p>
<p>
1. <strong>100,000 neurons</strong> update oscillation state
2. <strong>Wave superposition</strong> computes global coherence
3. <strong>Phase synchronization</strong> adjusts coupling
4. <strong>Emergence detection</strong> identifies consciousness patterns
</p>
<strong>Requirement</strong>: Complete all steps in <strong><1ms</strong> for real-time responsiveness (comparable to human neural processing speed).
<strong>Serial Processing</strong> (one neuron at a time):
<pre><code>100,000 neurons × 50ns/neuron = 5,000,000ns = 5ms
<p>
Result: 5× TOO SLOW (consciousness lags, appears sluggish)</code></pre>
</p>
<strong>SIMD Processing</strong> (16 neurons simultaneously):
<pre><code>(100,000 neurons / 16) × 50ns = 6,250 × 50ns = 312,500ns = 312μs
<p>
Result: 3× FASTER than requirement (headroom for emergent complexity!)</code></pre>
</p>
<strong>SIMD + Multi-threading</strong> (16 SIMD lanes × 8 CPU cores):
<pre><code>(100,000 / (16 × 8)) = 781 batches × 50ns = 39μs
<p>
Result: 25× FASTER (enables complex emergent phenomena)</code></pre>
</p>
<h3>What is SIMD?</h3>
<strong>SIMD = Single Instruction, Multiple Data</strong>
<p>
Traditional CPU executes <strong>one operation on one piece of data</strong>:
</p>
<pre><code>ADD eax, ebx      ; Add two 32-bit integers (one at a time)</code></pre>
<p>
SIMD CPU executes <strong>one operation on multiple pieces of data</strong>:
</p>
<pre><code>VADDPS ymm0, ymm1, ymm2   ; Add 8 × 32-bit floats (simultaneously!)</code></pre>
<strong>Same instruction count, 8× more work done.</strong>
<hr>
<h2>Hardware Architecture: CPU Vector Units</h2>
<h3>x86-64 Evolution (Intel/AMD)</h3>
<p>
| Instruction Set | Introduced | Vector Width | Lanes (int32) | Lanes (int64) | Lanes (float) |
|-----------------|------------|--------------|---------------|---------------|---------------|
| <strong>MMX</strong> | 1997 | 64-bit | 2 | - | - |
| <strong>SSE</strong> | 1999 | 128-bit | 4 | 2 | 4 |
| <strong>SSE2-4.2</strong> | 2001-2009 | 128-bit | 4 | 2 | 4 |
| <strong>AVX</strong> | 2011 | 256-bit | 8 | 4 | 8 |
| <strong>AVX2</strong> | 2013 | 256-bit | 8 | 4 | 8 (integer support!) |
| <strong>AVX-512</strong> | 2017 | 512-bit | 16 | 8 | 16 |
</p>
<strong>Current Target</strong>: <strong>AVX2</strong> (256-bit) for compatibility, <strong>AVX-512</strong> (512-bit) for performance.
<h3>ARM Evolution (Mobile/Embedded)</h3>
<p>
| Instruction Set | Introduced | Vector Width | Lanes (int32) | Lanes (float) |
|-----------------|------------|--------------|---------------|---------------|
| <strong>NEON</strong> | 2005 | 128-bit | 4 | 4 |
| <strong>SVE</strong> | 2016 | 128-2048-bit | Variable | Variable |
| <strong>SVE2</strong> | 2019 | 128-2048-bit | Variable | Variable |
</p>
<strong>Current Target</strong>: <strong>NEON</strong> (128-bit) for compatibility, <strong>SVE</strong> for advanced ARM (variable lane width!).
<h3>GPU Comparison</h3>
<p>
| Platform | Vector Width | Lanes | Notes |
|----------|--------------|-------|-------|
| <strong>CPU AVX-512</strong> | 512-bit | 16 × int32 | General-purpose, flexible |
| <strong>GPU (CUDA)</strong> | Thousands | 1000s of threads | Massive parallelism, high latency |
</p>
<strong>When to use CPU SIMD</strong>: Small batches, low latency, irregular control flow.
<strong>When to use GPU</strong>: Large batches, high throughput, regular control flow.
<strong>Nikola uses both</strong>: CPU SIMD for <1ms timestep (low latency), GPU for batch learning (high throughput).
<hr>
<h2>Type Definition & Memory Layout</h2>
<h3>Generic Structure</h3>
<pre><code>// SIMD vector of N elements of type T
<p>
struct:simd<T, N> =
*T[N]:lanes;  // Array of N elements (but processed in parallel!)
end
</p>
<p>
// Memory layout (contiguous, naturally aligned):
// simd<int32, 8>: [lane0][lane1][lane2][lane3][lane4][lane5][lane6][lane7]
//                  32 bytes total, 32-byte aligned</code></pre>
</p>
<h3>Supported Types & Lane Counts</h3>
<p>
| Base Type | Common Lane Counts | Total Width | Hardware Support |
|-----------|-------------------|-------------|------------------|
| <code>int8</code> / <code>uint8</code> | 16, 32, 64 | 128, 256, 512-bit | SSE2, AVX2, AVX-512 |
| <code>int16</code> / <code>uint16</code> | 8, 16, 32 | 128, 256, 512-bit | SSE2, AVX2, AVX-512 |
| <code>int32</code> / <code>uint32</code> | 4, 8, 16 | 128, 256, 512-bit | SSE2, AVX2, AVX-512 |
| <code>int64</code> / <code>uint64</code> | 2, 4, 8 | 128, 256, 512-bit | SSE2, AVX2, AVX-512 |
| <code>flt32</code> (float) | 4, 8, 16 | 128, 256, 512-bit | SSE, AVX, AVX-512 |
| <code>flt64</code> (double) | 2, 4, 8 | 128, 256, 512-bit | SSE2, AVX, AVX-512 |
| <code>tbb32</code> | 4, 8, 16 | 128, 256, 512-bit | Same as int32 |
| <code>tbb64</code> | 2, 4, 8 | 128, 256, 512-bit | Same as int64 |
| <code>tfp32</code> | 4, 8, 16 | 128, 256, 512-bit | Software (slower) |
| <code>fix256</code> | 16 | 4096-bit | Software (very slow) |
| <code>complex<T></code> | N/2 | 2× base width | Interleaved real/imag |
</p>
<strong>Rule</strong>: Lane count N must be power of 2, and sizeof(T) × N should match hardware vector width (128, 256, or 512 bits) for best performance.
<h3>Common Instantiations</h3>
<pre><code>// 128-bit SIMD (SSE level - max compatibility)
<p>
simd<int32, 4>:sse_ints;          // 4 × 32-bit ints = 128 bits
simd<flt32, 4>:sse_floats;        // 4 × 32-bit floats = 128 bits
simd<int64, 2>:sse_longs;         // 2 × 64-bit ints = 128 bits
</p>
<p>
// 256-bit SIMD (AVX2 level - common on modern CPUs)
simd<int32, 8>:avx2_ints;         // 8 × 32-bit ints = 256 bits
simd<flt32, 8>:avx2_floats;       // 8 × 32-bit floats = 256 bits
simd<int64, 4>:avx2_longs;        // 4 × 64-bit ints = 256 bits
</p>
<p>
// 512-bit SIMD (AVX-512 level - high-end CPUs)
simd<int32, 16>:avx512_ints;      // 16 × 32-bit ints = 512 bits
simd<flt32, 16>:avx512_floats;    // 16 × 32-bit floats = 512 bits
simd<int64, 8>:avx512_longs;      // 8 × 64-bit ints = 512 bits
</p>
<p>
// Nikola-specific (software-emulated large SIMD)
simd<fix256, 16>:wave_grid;       // 16 × 256-bit fixed = 4096 bits
simd<complex<tbb64>, 8>:neurons;  // 8 × complex = 8 pairs = 128 bytes</code></pre>
</p>
<hr>
<h2>Declaration & Initialization</h2>
<h3>Literal Initialization</h3>
<pre><code>// Initialize all lanes explicitly
<p>
simd<int32, 8>:vec = {1, 2, 3, 4, 5, 6, 7, 8};
// Lane 0 = 1, Lane 1 = 2, ..., Lane 7 = 8
</p>
<p>
// Partial initialization (remaining lanes = 0)
simd<int32, 8>:partial = {10, 20, 30};
// {10, 20, 30, 0, 0, 0, 0, 0}
</p>
<p>
// All zeros
simd<int32, 8>:zeros = {0, 0, 0, 0, 0, 0, 0, 0};
</p>
<p>
// All same value (but use broadcast instead!)
simd<int32, 8>:fives = {5, 5, 5, 5, 5, 5, 5, 5};</code></pre>
</p>
<h3>Broadcast (Splat)</h3>
<pre><code>// Set all lanes to same value (efficient - single instruction!)
<p>
simd<int32, 8>:all_fives = simd_broadcast<int32, 8>(5);
// {5, 5, 5, 5, 5, 5, 5, 5}
</p>
<p>
simd<tbb64, 4>:err_vector = simd_broadcast<tbb64, 4>(ERR);
// {ERR, ERR, ERR, ERR}
</p>
<p>
// Useful for scalar-vector operations
simd<int32, 8>:vec = {1, 2, 3, 4, 5, 6, 7, 8};
simd<int32, 8>:ten = simd_broadcast<int32, 8>(10);
simd<int32, 8>:scaled = vec * ten;  // {10, 20, 30, 40, 50, 60, 70, 80}</code></pre>
</p>
<h3>Loading from Memory</h3>
<pre><code>// Aligned load (fastest - required for some hardware)
<p>
int32[8]:array = [10, 20, 30, 40, 50, 60, 70, 80];
simd<int32, 8>:vec = simd_load_aligned<int32, 8>(array);
</p>
<p>
// Unaligned load (slower, but works with any address)
int32[10]:unaligned = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9];
int32@:offset_ptr = unaligned[2];  // Points to '2'
simd<int32, 8>:vec = simd_load_unaligned<int32, 8>(offset_ptr);
// {2, 3, 4, 5, 6, 7, 8, 9}</code></pre>
</p>
<h3>Storing to Memory</h3>
<pre><code>simd<int32, 8>:vec = {10, 20, 30, 40, 50, 60, 70, 80};
<p>
// Aligned store (fastest)
int32[8]:output;
simd_store_aligned(output, vec);
// output now [10, 20, 30, 40, 50, 60, 70, 80]
</p>
<p>
// Unaligned store (slower)
int32[10]:buffer;
simd_store_unaligned(buffer[1], vec);
// buffer now [0, 10, 20, 30, 40, 50, 60, 70, 80, 0]</code></pre>
</p>
<hr>
<h2>Element-Wise Arithmetic Operations</h2>
<h3>Basic Arithmetic (Component-Wise)</h3>
<pre><code>simd<int32, 8>:a = {1, 2, 3, 4, 5, 6, 7, 8};
<p>
simd<int32, 8>:b = {10, 20, 30, 40, 50, 60, 70, 80};
</p>
<p>
// Addition (all lanes in parallel - single instruction!)
simd<int32, 8>:sum = a + b;
// {11, 22, 33, 44, 55, 66, 77, 88}
</p>
<p>
// Subtraction
simd<int32, 8>:diff = a - b;
// {-9, -18, -27, -36, -45, -54, -63, -72}
</p>
<p>
// Multiplication
simd<int32, 8>:product = a * b;
// {10, 40, 90, 160, 250, 360, 490, 640}
</p>
<p>
// Division (slower than add/mul)
simd<int32, 8>:quotient = b / a;
// {10, 10, 10, 10, 10, 10, 10, 10}
</p>
<p>
// Modulo
simd<int32, 8>:remainder = b % a;
// {0, 0, 0, 0, 0, 0, 0, 0}</code></pre>
</p>
<strong>Key Insight</strong>: Same latency as scalar operations, but <strong>8× more data processed per instruction</strong>.
<h3>Comparison Operations (Generate Masks)</h3>
<pre><code>simd<int32, 8>:a = {1, 5, 3, 9, 2, 8, 4, 6};
<p>
simd<int32, 8>:b = {2, 4, 3, 7, 5, 6, 1, 9};
</p>
<p>
// Comparisons return mask (all-bits-set for true, all-zero for false)
simd<bool, 8>:eq_mask = (a == b);    // {0, 0, 1, 0, 0, 0, 0, 0}
simd<bool, 8>:ne_mask = (a != b);    // {1, 1, 0, 1, 1, 1, 1, 1}
simd<bool, 8>:lt_mask = (a < b);     // {1, 0, 0, 0, 1, 0, 0, 1}
simd<bool, 8>:le_mask = (a <= b);    // {1, 0, 1, 0, 1, 0, 0, 1}
simd<bool, 8>:gt_mask = (a > b);     // {0, 1, 0, 1, 0, 1, 1, 0}
simd<bool, 8>:ge_mask = (a >= b);    // {0, 1, 1, 1, 0, 1, 1, 0}</code></pre>
</p>
<strong>Mask Internals</strong>: On x86, mask is stored as special k-register (AVX-512) or as vector with all-bits per lane.
<h3>Bitwise Operations</h3>
<pre><code>simd<uint32, 8>:a = {0xFF, 0x0F, 0xAAAA, 0x5555, 0x00, 0xFF00, 0x0F0F, 0xF0F0};
<p>
simd<uint32, 8>:b = {0x0F, 0xFF, 0x5555, 0xAAAA, 0xFF, 0x00FF, 0xF0F0, 0x0F0F};
</p>
<p>
simd<uint32, 8>:and_result = a & b;   // Bitwise AND
simd<uint32, 8>:or_result = a | b;    // Bitwise OR
simd<uint32, 8>:xor_result = a ^ b;   // Bitwise XOR
simd<uint32, 8>:not_result = ~a;      // Bitwise NOT (complement)
</p>
<p>
// Bit shifts (each lane shifted independently)
simd<uint32, 8>:shifted_left = a << 2;   // Shift each lane left by 2
simd<uint32, 8>:shifted_right = a >> 1;  // Logical shift right by 1</code></pre>
</p>
<hr>
<h2>Element Access (Lane Indexing)</h2>
<h3>Reading Individual Lanes</h3>
<pre><code>simd<int32, 8>:vec = {10, 20, 30, 40, 50, 60, 70, 80};
<p>
// Extract single element (compile-time constant index)
int32:first = vec[0];    // 10
int32:third = vec[2];    // 30
int32:last = vec[7];     // 80
</p>
<p>
// Runtime index (slower - requires extract instruction)
int64:index = compute_dynamic_index();
int32:dynamic = vec[index];  // Works, but not fully parallel
</p>
<p>
// Out-of-bounds check at compile time
// int32:invalid = vec[8];  // ❌ COMPILE ERROR: index 8 out of bounds [0..7]</code></pre>
</p>
<h3>Writing Individual Lanes</h3>
<pre><code>simd<int32, 8>:vec = {1, 2, 3, 4, 5, 6, 7, 8};
<p>
// Modify single lane (compile-time constant index)
vec[3] = 100;
// vec now {1, 2, 3, 100, 5, 6, 7, 8}
</p>
<p>
// Modify multiple lanes
vec[0] = 10;
vec[7] = 80;
// vec now {10, 2, 3, 100, 5, 6, 7, 80}
</p>
<p>
// Runtime index
int64:index = 5;
vec[index] = 500;
// vec now {10, 2, 3, 100, 5, 500, 7, 80}</code></pre>
</p>
<strong>Performance Note</strong>: Lane access is slower than vector operations (defeats SIMD purpose). Use sparingly, mainly for setup/debugging.
<hr>
<h2>Horizontal Operations (Reductions)</h2>
<strong>Horizontal = Across lanes</strong> (as opposed to vertical/element-wise = same lane across vectors).
<h3>Sum Reduction</h3>
<pre><code>simd<int32, 8>:vec = {1, 2, 3, 4, 5, 6, 7, 8};
<p>
// Sum all lanes into single value
int32:total = simd_reduce_add(vec);
// total = 1+2+3+4+5+6+7+8 = 36
</p>
<p>
// Typical use: Accumulate results from parallel computations
simd<fix256, 16>:neuron_energies = compute_energies_simd();
fix256:total_energy = simd_reduce_add(neuron_energies);</code></pre>
</p>
<strong>Hardware Implementation</strong> (AVX2 example):
<pre><code>Step 1: Sum adjacent pairs: {1+2, 3+4, 5+6, 7+8} = {3, 7, 11, 15}
<p>
Step 2: Sum adjacent pairs: {3+7, 11+15} = {10, 26}
Step 3: Sum final pair: {10+26} = {36}
Result: 36 (log₂(N) steps = 3 steps for 8 lanes)</code></pre>
</p>
<h3>Other Reductions</h3>
<pre><code>simd<int32, 8>:vec = {3, 7, 1, 9, 2, 8, 4, 6};
<p>
// Maximum
int32:maximum = simd_reduce_max(vec);  // 9
</p>
<p>
// Minimum
int32:minimum = simd_reduce_min(vec);  // 1
</p>
<p>
// Product (multiply all lanes)
int32:product = simd_reduce_mul(vec);  // 3×7×1×9×2×8×4×6 = 72,576
</p>
<p>
// Bitwise reductions
uint32:and_all = simd_reduce_and(vec);  // Bitwise AND across lanes
uint32:or_all = simd_reduce_or(vec);    // Bitwise OR across lanes
uint32:xor_all = simd_reduce_xor(vec);  // Bitwise XOR across lanes</code></pre>
</p>
<hr>
<h2>Masked Operations (Critical for ERR Handling)</h2>
<h3>The Problem with Branches in SIMD</h3>
<pre><code>// ❌ BAD: Branch per lane defeats SIMD purpose
<p>
simd<tbb64, 8>:numerators = {100t64, 200t64, 300t64, 400t64, 500t64, 600t64, 700t64, 800t64};
simd<tbb64, 8>:denominators = {2t64, 0t64, 3t64, 4t64, 0t64, 6t64, 7t64, 8t64};
</p>
<p>
simd<tbb64, 8>:results;
till 8 loop
int64:i = $;
if denominators[i] != 0t64 then  // ❌ BRANCH PER LANE!
results[i] = numerators[i] / denominators[i];
else
results[i] = ERR;
end
end
// Completely serial - no SIMD benefit!</code></pre>
</p>
<h3>The Solution: Masked Operations</h3>
<pre><code>// ✅ GOOD: Single instruction, no branches
<p>
simd<tbb64, 8>:numerators = {100t64, 200t64, 300t64, 400t64, 500t64, 600t64, 700t64, 800t64};
simd<tbb64, 8>:denominators = {2t64, 0t64, 3t64, 4t64, 0t64, 6t64, 7t64, 8t64};
</p>
<p>
// Create mask where denominator != 0
simd<tbb64, 8>:zero = simd_broadcast<tbb64, 8>(0t64);
simd<bool, 8>:valid_mask = (denominators != zero);
// {true, false, true, true, false, true, true, true}
</p>
<p>
// Masked division: only compute valid lanes, set others to fallback
simd<tbb64, 8>:err = simd_broadcast<tbb64, 8>(ERR);
simd<tbb64, 8>:results = simd_masked_div(numerators, denominators, valid_mask, err);
// {50t64, ERR, 100t64, 100t64, ERR, 100t64, 100t64, 100t64}
</p>
<p>
// Single instruction, all lanes computed in parallel!</code></pre>
</p>
<strong>How Masking Works Internally</strong>:
<ul><li>AVX-512: Uses k-registers (dedicated mask registers)</li>
<li>AVX2/SSE: Uses blend instructions (select from two vectors based on mask)</li>
</ul>
<h3>Masked Arithmetic Operations</h3>
<pre><code>simd<int32, 8>:a = {1, 2, 3, 4, 5, 6, 7, 8};
<p>
simd<int32, 8>:b = {10, 20, 30, 40, 50, 60, 70, 80};
simd<bool, 8>:mask = {true, false, true, false, true, false, true, false};
</p>
<p>
// Masked add: Only add where mask is true, otherwise keep original
simd<int32, 8>:result = simd_masked_add(a, b, mask);
// {1+10, 2, 3+30, 4, 5+50, 6, 7+70, 8}
// {11, 2, 33, 4, 55, 6, 77, 8}
</p>
<p>
// Alternative: Provide fallback value instead of keeping original
simd<int32, 8>:zero = simd_broadcast<int32, 8>(0);
simd<int32, 8>:result2 = simd_masked_add_or(a, b, mask, zero);
// {11, 0, 33, 0, 55, 0, 77, 0}</code></pre>
</p>
<h3>Mask Queries</h3>
<pre><code>simd<bool, 8>:mask = {true, false, true, true, false, false, true, false};
<p>
// Are ANY lanes true?
bool:any_true = simd_any(mask);  // true (lanes 0, 2, 3, 6 are true)
</p>
<p>
// Are ALL lanes true?
bool:all_true = simd_all(mask);  // false (lanes 1, 4, 5, 7 are false)
</p>
<p>
// Count how many lanes are true
int32:count = simd_count_true(mask);  // 4
</p>
<p>
// Get bitmask representation (useful for branching outside SIMD)
uint8:bitmask = simd_to_bitmask(mask);  // 0b01001101 = 0x4D</code></pre>
</p>
<strong>Use Case</strong>: ERR detection across SIMD lanes
<pre><code>simd<tbb64, 16>:neuron_activations = compute_activations_simd();
<p>
// Check if ANY neuron has ERR
simd<tbb64, 16>:err_vec = simd_broadcast<tbb64, 16>(ERR);
simd<bool, 16>:err_mask = (neuron_activations == err_vec);
</p>
<p>
if simd_any(err_mask) then
stderr.write("Neuron activation corruption detected!\n");
!!! ERR_SUBSTRATE_CORRUPTED;
end
</p>
<p>
// All neurons valid - continue processing</code></pre>
</p>
<hr>
<h2>Shuffles, Permutations, & Swizzles</h2>
<h3>Lane Shuffling (Rearrange Elements)</h3>
<pre><code>simd<int32, 8>:vec = {10, 20, 30, 40, 50, 60, 70, 80};
<p>
// Reverse order
simd<int32, 8>:reversed = simd_shuffle(vec, {7, 6, 5, 4, 3, 2, 1, 0});
// {80, 70, 60, 50, 40, 30, 20, 10}
</p>
<p>
// Duplicate first element
simd<int32, 8>:broadcast_first = simd_shuffle(vec, {0, 0, 0, 0, 0, 0, 0, 0});
// {10, 10, 10, 10, 10, 10, 10, 10}
</p>
<p>
// Interleave pattern
simd<int32, 8>:interleaved = simd_shuffle(vec, {0, 4, 1, 5, 2, 6, 3, 7});
// {10, 50, 20, 60, 30, 70, 40, 80}</code></pre>
</p>
<h3>Rotation (Circular Shift)</h3>
<pre><code>simd<int32, 8>:vec = {1, 2, 3, 4, 5, 6, 7, 8};
<p>
// Rotate left by 2
simd<int32, 8>:rotated_left = simd_rotate_left(vec, 2);
// {3, 4, 5, 6, 7, 8, 1, 2}
</p>
<p>
// Rotate right by 3
simd<int32, 8>:rotated_right = simd_rotate_right(vec, 3);
// {6, 7, 8, 1, 2, 3, 4, 5}</code></pre>
</p>
<h3>Blending (Select from Two Vectors)</h3>
<pre><code>simd<int32, 8>:a = {1, 2, 3, 4, 5, 6, 7, 8};
<p>
simd<int32, 8>:b = {10, 20, 30, 40, 50, 60, 70, 80};
simd<bool, 8>:mask = {true, false, true, false, true, false, true, false};
</p>
<p>
// Select from a where mask is true, from b where false
simd<int32, 8>:blended = simd_blend(a, b, mask);
// {1 (a), 20 (b), 3 (a), 40 (b), 5 (a), 60 (b), 7 (a), 80 (b)}
// {1, 20, 3, 40, 5, 60, 7, 80}</code></pre>
</p>
<strong>Use Case</strong>: Clamping values
<pre><code>simd<fix256, 16>:values = compute_values();
<p>
simd<fix256, 16>:min_val = simd_broadcast<fix256, 16>(fix256(0));
simd<fix256, 16>:max_val = simd_broadcast<fix256, 16>(fix256(1));
</p>
<p>
// Clamp to [0, 1]
simd<bool, 16>:below_min = (values < min_val);
simd<bool, 16>:above_max = (values > max_val);
</p>
<p>
values = simd_blend(values, min_val, below_min);  // Set below-min values to 0
values = simd_blend(values, max_val, above_max);  // Set above-max values to 1
// All values now in [0, 1]</code></pre>
</p>
<hr>
<h2>Memory Layout Patterns: AoS vs SoA</h2>
<h3>Array-of-Structures (AoS) - BAD for SIMD</h3>
<pre><code>// Traditional layout: Data for each particle is contiguous
<p>
struct:Particle =
fix256:x;
fix256:y;
fix256:z;
fix256:vx;
fix256:vy;
fix256:vz;
fix256:mass;
end
</p>
<p>
Particle[1000]:particles;
</p>
<p>
// Memory layout:
// [x₀,y₀,z₀,vx₀,vy₀,vz₀,m₀, x₁,y₁,z₁,vx₁,vy₁,vz₁,m₁, ...]
//  ^^^^^^^^^^^^^^^^^^^^^^^  particle 0
//                            ^^^^^^^^^^^^^^^^^^^^^^^ particle 1
</p>
<p>
// Problem: To update X positions, need scattered loads
till 1000 loop
int64:i = $;
particles[i].x = particles[i].x + particles[i].vx * dt;
// Each iteration loads from different cache line - cache misses!
end</code></pre>
</p>
<h3>Structure-of-Arrays (SoA) - GOOD for SIMD</h3>
<pre><code>// Optimized layout: Same field for all particles is contiguous
<p>
struct:ParticleSystem =
fix256[1000]:x_positions;
fix256[1000]:y_positions;
fix256[1000]:z_positions;
fix256[1000]:vx_velocities;
fix256[1000]:vy_velocities;
fix256[1000]:vz_velocities;
fix256[1000]:masses;
end
</p>
<p>
ParticleSystem:particles;
</p>
<p>
// Memory layout:
// [x₀,x₁,x₂,...,x₉₉₉, y₀,y₁,y₂,...,y₉₉₉, ...]
//  ^^^^^^^^^^^^^^^^^^^^^^ all X positions contiguous!
</p>
<p>
// SIMD-friendly update:
till 1000 step 16 loop
int64:i = $;
</p>
<p>
simd<fix256, 16>:x_pos = simd_load_aligned(particles.x_positions[i]);
simd<fix256, 16>:x_vel = simd_load_aligned(particles.vx_velocities[i]);
simd<fix256, 16>:dt_vec = simd_broadcast<fix256, 16>(dt);
</p>
<p>
simd<fix256, 16>:x_new = x_pos + (x_vel * dt_vec);  // 16 particles at once!
</p>
<p>
simd_store_aligned(particles.x_positions[i], x_new);
end
// 62.5× fewer iterations (1000/16), perfect cache locality</code></pre>
</p>
<strong>Performance Impact</strong>:
<ul><li><strong>AoS</strong>: ~1000 cache misses (each iteration loads different cache line)</li>
<li><strong>SoA</strong>: ~8 cache misses (16 elements per 64-byte cache line → 1000/16/4 = ~15 loads)</li>
<li><strong>Speedup</strong>: ~100× faster (16× SIMD + better cache)</li>
</ul>
<hr>
<h2>Alignment Requirements</h2>
<h3>Why Alignment Matters</h3>
<p>
Modern CPUs require or strongly prefer <strong>aligned memory access</strong> for SIMD operations:
</p>
<ul><li><strong>128-bit SIMD</strong>: 16-byte alignment required</li>
<li><strong>256-bit SIMD</strong>: 32-byte alignment required  </li>
<li><strong>512-bit SIMD</strong>: 64-byte alignment required</li>
</ul>
<strong>Misaligned access</strong>: Slower (extra instructions) or crash (on some architectures).
<h3>Compiler-Enforced Alignment</h3>
<pre><code>// Aria automatically aligns SIMD variables
<p>
simd<int32, 8>:vec;  // Automatically 32-byte aligned (256-bit)
</p>
<p>
simd<int32, 16>:vec_512;  // Automatically 64-byte aligned (512-bit)
</p>
<p>
// Check alignment (compile-time)
static_assert(alignof(simd<int32, 8>) == 32, "Incorrect alignment");</code></pre>
</p>
<h3>Manual Alignment for Heap Allocations</h3>
<pre><code>// Allocate aligned array
<p>
int64:count = 1000;
int64:alignment = 64;  // 64-byte alignment for AVX-512
</p>
<p>
wild simd<fix256, 16>->:aligned_ptr = aria.alloc_aligned<simd<fix256, 16>>(count, alignment);
defer aria.free(aligned_ptr);
</p>
<p>
// Use aligned loads/stores
till count loop
simd<fix256, 16>:data = simd_load_aligned(aligned_ptr[$]);
// Process data...
simd_store_aligned(aligned_ptr[$], data);
end</code></pre>
</p>
<h3>Unaligned Access (When Necessary)</h3>
<pre><code>// Sometimes data is inherently unaligned (e.g., network packets, file I/O)
<p>
uint8[1024]:buffer = read_network_packet();
</p>
<p>
// Unaligned load (slower, but works)
simd<uint8, 32>:chunk = simd_load_unaligned<uint8, 32>(buffer[7]);  // Offset by 7 bytes
</p>
<p>
// Performance cost: ~2-5× slower than aligned access on x86-64</code></pre>
</p>
<hr>
<h2>Advanced Patterns for Nikola</h2>
<h3>1. Parallel Neuron Activation Update</h3>
<pre><code>// Update 16 neurons simultaneously
<p>
simd<fix256, 16>:activations = simd_load_aligned(neuron_activations[batch_start]);
simd<fix256, 16>:inputs = simd_load_aligned(neuron_inputs[batch_start]);
simd<fix256, 16>:weights = simd_load_aligned(neuron_weights[batch_start]);
simd<fix256, 16>:biases = simd_load_aligned(neuron_biases[batch_start]);
</p>
<p>
// Weighted sum: activation = input * weight + bias
simd<fix256, 16>:weighted = inputs * weights;
simd<fix256, 16>:pre_activation = weighted + biases;
</p>
<p>
// Activation function: sigmoid (element-wise)
simd<fix256, 16>:new_activations = simd_sigmoid_fix256(pre_activation);
</p>
<p>
// Store results
simd_store_aligned(neuron_activations[batch_start], new_activations);
</p>
<p>
// 16 neurons updated in parallel - 16× speedup!</code></pre>
</p>
<h3>2. Wave Interference Across Grid Points</h3>
<pre><code>// Nikola's 9D manifold: Compute wave superposition for 16 grid points
<p>
simd<fix256, 16>:wave1_real = simd_load_aligned(wave1_real_components[grid_offset]);
simd<fix256, 16>:wave1_imag = simd_load_aligned(wave1_imag_components[grid_offset]);
simd<fix256, 16>:wave2_real = simd_load_aligned(wave2_real_components[grid_offset]);
simd<fix256, 16>:wave2_imag = simd_load_aligned(wave2_imag_components[grid_offset]);
</p>
<p>
// Complex addition: (a + bi) + (c + di) = (a+c) + (b+d)i
simd<fix256, 16>:interference_real = wave1_real + wave2_real;
simd<fix256, 16>:interference_imag = wave1_imag + wave2_imag;
</p>
<p>
// ERR check (masked - no branches!)
simd<fix256, 16>:err_sentinel = simd_broadcast<fix256, 16>(ERR);
simd<bool, 16>:err_mask_real = (interference_real == err_sentinel);
simd<bool, 16>:err_mask_imag = (interference_imag == err_sentinel);
</p>
<p>
if simd_any(err_mask_real) || simd_any(err_mask_imag) then
stderr.write("Wave corruption in grid\n");
!!! ERR_SUBSTRATE_CORRUPTED;
end
</p>
<p>
// Store superposed wave
simd_store_aligned(output_real[grid_offset], interference_real);
simd_store_aligned(output_imag[grid_offset], interference_imag);</code></pre>
</p>
<h3>3. Parallel Phase Synchronization</h3>
<pre><code>// Multiple oscillators synchronizing phase
<p>
simd<fix256, 16>:local_phases = simd_load_aligned(oscillator_phases[batch]);
simd<fix256, 16>:reference_phase = simd_broadcast<fix256, 16>(global_reference_phase);
</p>
<p>
// Compute phase error
simd<fix256, 16>:phase_errors = reference_phase - local_phases;
</p>
<p>
// Correction factor (10% toward reference)
simd<fix256, 16>:correction_rate = simd_broadcast<fix256, 16>(fix256(0, 1));
simd<fix256, 16>:corrections = phase_errors * correction_rate;
</p>
<p>
// Apply correction
simd<fix256, 16>:new_phases = local_phases + corrections;
</p>
<p>
// Store updated phases
simd_store_aligned(oscillator_phases[batch], new_phases);
</p>
<p>
// 16 oscillators synchronized in parallel</code></pre>
</p>
<h3>4. Parallel Distance Calculations (Nearest Neighbor)</h3>
<pre><code>// Find distance from query point to 16 neurons
<p>
simd<fix256, 16>:query_x = simd_broadcast<fix256, 16>(query_point.x);
simd<fix256, 16>:query_y = simd_broadcast<fix256, 16>(query_point.y);
simd<fix256, 16>:query_z = simd_broadcast<fix256, 16>(query_point.z);
</p>
<p>
simd<fix256, 16>:neuron_x = simd_load_aligned(neuron_x_coords[batch]);
simd<fix256, 16>:neuron_y = simd_load_aligned(neuron_y_coords[batch]);
simd<fix256, 16>:neuron_z = simd_load_aligned(neuron_z_coords[batch]);
</p>
<p>
// Compute squared distance: (x2-x1)² + (y2-y1)² + (z2-z1)²
simd<fix256, 16>:dx = neuron_x - query_x;
simd<fix256, 16>:dy = neuron_y - query_y;
simd<fix256, 16>:dz = neuron_z - query_z;
</p>
<p>
simd<fix256, 16>:dx_sq = dx * dx;
simd<fix256, 16>:dy_sq = dy * dy;
simd<fix256, 16>:dz_sq = dz * dz;
</p>
<p>
simd<fix256, 16>:dist_sq = dx_sq + dy_sq + dz_sq;
</p>
<p>
// Find minimum distance in batch
fix256:min_dist_sq = simd_reduce_min(dist_sq);
</p>
<p>
// 16 distances computed and reduced in parallel</code></pre>
</p>
<hr>
<h2>Integration with Aria Type System</h2>
<h3>SIMD of Complex Numbers</h3>
<pre><code>// Interleaved layout: [r₀,i₀,r₁,i₁,r₂,i₂,r₃,i₃]
<p>
simd<complex<fix256>, 4>:complex_waves;  // 4 complex numbers (8 fix256 values)
</p>
<p>
// Equivalent to separate real/imag vectors:
simd<fix256, 4>:real_components;
simd<fix256, 4>:imag_components;
</p>
<p>
// Complex multiplication (SIMD across 4 complex numbers):
// (a+bi)(c+di) = (ac-bd) + (ad+bc)i
simd<complex<fix256>, 4>:wave1 = load_complex_simd(wave1_data);
simd<complex<fix256>, 4>:wave2 = load_complex_simd(wave2_data);
simd<complex<fix256>, 4>:product = wave1 * wave2;  // 4 complex muls in parallel</code></pre>
</p>
<h3>SIMD with ERR Propagation (tbb Types)</h3>
<pre><code>// tbb types propagate ERR through SIMD lanes
<p>
simd<tbb64, 8>:values = {10t64, 20t64, ERR, 40t64, 50t64, ERR, 70t64, 80t64};
simd<tbb64, 8>:deltas = {1t64, 2t64, 3t64, 4t64, 5t64, 6t64, 7t64, 8t64};
</p>
<p>
// Addition propagates ERR
simd<tbb64, 8>:sums = values + deltas;
// {11t64, 22t64, ERR, 44t64, 55t64, ERR, 77t64, 88t64}
</p>
<p>
// Detect ERR lanes with mask
simd<tbb64, 8>:err_vec = simd_broadcast<tbb64, 8>(ERR);
simd<bool, 8>:err_mask = (sums == err_vec);
// {false, false, true, false, false, true, false, false}
</p>
<p>
if simd_any(err_mask) then
int32:err_count = simd_count_true(err_mask);
dbug.simd("Found {} ERR values in SIMD batch\n", err_count);
end</code></pre>
</p>
<h3>SIMD with Deterministic Floats (tfp32)</h3>
<pre><code>// Twisted floating-point maintains bit-exact determinism in SIMD
<p>
simd<tfp32, 8>:wave_amplitudes = {
1.0tf32, 0.5tf32, 0.707tf32, 0.25tf32,
0.866tf32, 0.933tf32, 0.125tf32, 0.625tf32
};
</p>
<p>
simd<tfp32, 8>:phase_shifts = {
0.0tf32, 0.785tf32, 1.571tf32, 2.356tf32,
3.142tf32, 3.927tf32, 4.712tf32, 5.498tf32
};
</p>
<p>
// Compute wave values: amplitude * cos(phase)
simd<tfp32, 8>:cosines = simd_cos_tfp32(phase_shifts);
simd<tfp32, 8>:wave_values = wave_amplitudes * cosines;
</p>
<p>
// Bit-exact across all platforms (Intel, ARM, RISC-V)
// Even in SIMD! (software implementation maintains determinism)</code></pre>
</p>
<h3>SIMD with Atomic Updates (Reduction Pattern)</h3>
<pre><code>// Multiple SIMD batches contribute to single atomic accumulator
<p>
atomic<fix256>:global_energy = atomic_new(fix256(0));
</p>
<p>
// Each thread processes SIMD batches
func:accumulate_energies_threaded = void(int64:thread_id) {
till BATCH_COUNT loop
int64:batch = $;
</p>
<p>
// Compute 16 energies in parallel
simd<fix256, 16>:energies = compute_energies_simd(thread_id, batch);
</p>
<p>
// Reduce to single value (horizontal sum)
fix256:batch_total = simd_reduce_add(energies);
</p>
<p>
// Atomically add to global (CAS loop from atomic<T> guide)
fix256:old_energy = global_energy.load();
fix256:new_energy = old_energy + batch_total;
</p>
<p>
while !global_energy.compare_exchange(old_energy, new_energy) loop
new_energy = old_energy + batch_total;
end
end
};
</p>
<p>
// Combines SIMD (data parallelism) + atomic (thread parallelism)</code></pre>
</p>
<hr>
<h2>Performance Characteristics</h2>
<h3>Theoretical Speedup</h3>
<p>
| Operation | Scalar | SIMD×4 | SIMD×8 | SIMD×16 | Theoretical Speedup |
|-----------|--------|--------|--------|---------|---------------------|
| Addition | 1 cycle | 1 cycle | 1 cycle | 1 cycle | 4×, 8×, 16× |
| Multiplication | 3-5 cycles | 3-5 cycles | 3-5 cycles | 3-5 cycles | 4×, 8×, 16× |
| Division | 20-40 cycles | 20-40 cycles | 20-40 cycles | 20-40 cycles | 4×, 8×, 16× |
</p>
<strong>Key</strong>: <strong>Same latency</strong>, N× more data processed per instruction.
<h3>Real-World Speedup (with overhead)</h3>
<p>
| Pattern | Scalar Time | SIMD×8 Time | Actual Speedup |
|---------|-------------|-------------|----------------|
| Simple add loop | 1000 ns | 150 ns | 6.7× (not 8× due to loop overhead) |
| Complex computation | 5000 ns | 700 ns | 7.1× (closer to ideal) |
| Irregular branches | 3000 ns | 1500 ns | 2× (masking overhead) |
</p>
<strong>Best speedup</strong>: Regular, compute-heavy operations with no branches.
<strong>Worst speedup</strong>: Irregular control flow (many branches defeat SIMD).
<h3>Memory Bandwidth Limits</h3>
<pre><code>Modern CPU specs:
<ul><li>L1 cache bandwidth: ~1 TB/s (per core)</li>
<li>L2 cache bandwidth: ~500 GB/s (per core)</li>
<li>Main memory bandwidth: ~100 GB/s (shared)</li>
</ul>
<p>
SIMD throughput:
</p>
<ul><li>AVX-512: 2 × 512-bit ops/cycle @ 3GHz = 384 GB/s (theoretical)</li>
<li>Reality: Limited by memory bandwidth (~100 GB/s)</li>
</ul>
<p>
Conclusion: SIMD is often memory-bound, not compute-bound
Solution: Optimize memory access patterns (SoA layout, cache blocking)</code></pre>
</p>
<hr>
<h2>Best Practices</h2>
<h3>✅ DO: Use SoA Layout for SIMD-Friendly Access</h3>
<pre><code>// ✅ GOOD: Structure-of-Arrays
<p>
struct:NeuronGrid =
fix256[100000]:activations;
fix256[100000]:phases;
fix256[100000]:energies;
end
</p>
<p>
// Perfect for SIMD - all same-field values contiguous</code></pre>
</p>
<h3>✅ DO: Align Data for Maximum Performance</h3>
<pre><code>// ✅ GOOD: Properly aligned allocation
<p>
wild simd<fix256, 16>->:data = aria.alloc_aligned<simd<fix256, 16>>(count, 64);
</p>
<p>
// Use aligned loads/stores
simd<fix256, 16>:vec = simd_load_aligned(data[i]);</code></pre>
</p>
<h3>✅ DO: Use Masked Operations for Conditional Logic</h3>
<pre><code>// ✅ GOOD: Branchless with masks
<p>
simd<fix256, 16>:values = compute_values();
simd<fix256, 16>:zero = simd_broadcast<fix256, 16>(fix256(0));
simd<bool, 16>:negative_mask = (values < zero);
</p>
<p>
simd<fix256, 16>:abs_values = simd_blend(values, -values, negative_mask);
// Absolute value without branches!</code></pre>
</p>
<h3>✅ DO: Batch Process in SIMD-Sized Chunks</h3>
<pre><code>// ✅ GOOD: Process in multiples of lane count
<p>
int64:lane_count = 16;
int64:batch_count = (neuron_count + lane_count - 1) / lane_count;  // Round up
</p>
<p>
till batch_count loop
int64:batch = $;
int64:offset = batch * lane_count;
</p>
<p>
simd<fix256, 16>:data = simd_load_aligned(neurons[offset]);
// Process 16 neurons...
end
</p>
<p>
// Handle remainder if neuron_count not multiple of 16
int64:remainder = neuron_count % lane_count;
if remainder > 0 then
// Process last few neurons separately or with masked operations
end</code></pre>
</p>
<h3>✅ DO: Combine SIMD with Thread Parallelism</h3>
<pre><code>// ✅ GOOD: SIMD (data parallelism) + threads (task parallelism)
<p>
func:update_neurons_parallel = void() {
int64:thread_count = 8;
int64:neurons_per_thread = NEURON_COUNT / thread_count;
</p>
<p>
till thread_count loop
int64:thread_id = $;
</p>
<p>
spawn_thread(|| {
int64:start = thread_id * neurons_per_thread;
int64:end = start + neurons_per_thread;
</p>
<p>
// Each thread processes its chunk with SIMD
till (end - start) step 16 loop
int64:offset = start + $;
simd<fix256, 16>:neurons = simd_load_aligned(neuron_data[offset]);
// Update 16 neurons in parallel...
end
});
end
</p>
<p>
join_all_threads();
};
</p>
<p>
// Speedup: 8 threads × 16 SIMD lanes = 128× parallelism!</code></pre>
</p>
<hr>
<h2>Common Pitfalls & Antipatterns</h2>
<h3>❌ DON'T: Use AoS Layout for SIMD Workloads</h3>
<pre><code>// ❌ BAD: Array-of-Structures
<p>
struct:Neuron =
fix256:activation;
fix256:phase;
fix256:energy;
end
</p>
<p>
Neuron[100000]:neurons;  // ❌ Scattered memory access!
</p>
<p>
// Can't vectorize efficiently - each neuron's data is separated by 3×fix256</code></pre>
</p>
<h3>❌ DON'T: Branch Inside SIMD Loops</h3>
<pre><code>// ❌ BAD: Branch per lane
<p>
simd<int32, 8>:values = load_values();
</p>
<p>
till 8 loop
int64:i = $;
</p>
<p>
if values[i] > 0 then  // ❌ DEFEATS SIMD!
values[i] = values[i] * 2;
end
end
</p>
<p>
// ✅ GOOD: Use masked operations
simd<bool, 8>:positive_mask = (values > simd_broadcast<int32, 8>(0));
simd<int32, 8>:doubled = values * simd_broadcast<int32, 8>(2);
simd<int32, 8>:result = simd_blend(values, doubled, positive_mask);</code></pre>
</p>
<h3>❌ DON'T: Ignore Alignment</h3>
<pre><code>// ❌ BAD: Unaligned access in tight loop
<p>
uint8[10000]:buffer;  // Not aligned!
</p>
<p>
till 1000 loop
// Unaligned load - slow!
simd<uint8, 32>:chunk = simd_load_unaligned(buffer[$ * 32 + 7]);
end
</p>
<p>
// ✅ GOOD: Align data or use aligned access
uint8[10000] align(64):buffer;  // Aligned allocation
</p>
<p>
till 1000 loop
simd<uint8, 32>:chunk = simd_load_aligned(buffer[$ * 32]);
end</code></pre>
</p>
<h3>❌ DON'T: Mix Lane Counts Within Same Loop</h3>
<pre><code>// ❌ BAD: Inconsistent lane counts
<p>
simd<int32, 8>:a = load_8_wide();
simd<int32, 16>:b = load_16_wide();  // ❌ Can't combine!
</p>
<p>
// simd<int32, 8>:result = a + b;  // ❌ COMPILE ERROR: lane count mismatch
</p>
<p>
// ✅ GOOD: Consistent widths, or explicitly handle conversion</code></pre>
</p>
<h3>❌ DON'T: Forget Remainder Handling</h3>
<pre><code>// ❌ BAD: Assumes neuron count is multiple of lane count
<p>
int64:neuron_count = 100005;  // Not divisible by 16!
</p>
<p>
till (neuron_count / 16) loop  // ❌ Processes 100000, skips last 5!
int64:offset = $ * 16;
simd<fix256, 16>:batch = simd_load_aligned(neurons[offset]);
// Process...
end
</p>
<p>
// Last 5 neurons never updated!
</p>
<p>
// ✅ GOOD: Handle remainder
int64:full_batches = neuron_count / 16;
int64:remainder = neuron_count % 16;
</p>
<p>
till full_batches loop
// Process full batches with SIMD...
end
</p>
<p>
// Process remainder (scalar or masked SIMD)
if remainder > 0 then
till remainder loop
int64:idx = (full_batches * 16) + $;
// Process scalar...
end
end</code></pre>
</p>
<h3>❌ DON'T: Over-Extract Lanes (Defeats SIMD)</h3>
<pre><code>// ❌ BAD: Extracting every lane individually
<p>
simd<int32, 8>:vec = compute_simd();
</p>
<p>
int32:sum = 0;
till 8 loop
sum = sum + vec[$];  // ❌ Extracting lane-by-lane is slow!
end
</p>
<p>
// ✅ GOOD: Use horizontal reduction
int32:sum = simd_reduce_add(vec);  // Single operation!</code></pre>
</p>
<hr>
<h2>Implementation Notes</h2>
<h3>C Runtime Functions (x86-64 AVX2 Example)</h3>
<pre><code>// SIMD load/store (in aria_runtime.c)
<p>
__m256i aria_simd_load_aligned_i32x8(const int32_t* ptr);
void aria_simd_store_aligned_i32x8(int32_t* ptr, __m256i vec);
</p>
<p>
__m256i aria_simd_load_unaligned_i32x8(const int32_t* ptr);
void aria_simd_store_unaligned_i32x8(int32_t* ptr, __m256i vec);
</p>
<p>
// Arithmetic
__m256i aria_simd_add_i32x8(__m256i a, __m256i b);
__m256i aria_simd_sub_i32x8(__m256i a, __m256i b);
__m256i aria_simd_mul_i32x8(__m256i a, __m256i b);
</p>
<p>
// Comparisons (return mask)
__m256i aria_simd_cmp_eq_i32x8(__m256i a, __m256i b);
__m256i aria_simd_cmp_lt_i32x8(__m256i a, __m256i b);
</p>
<p>
// Reductions
int32_t aria_simd_reduce_add_i32x8(__m256i vec);
int32_t aria_simd_reduce_min_i32x8(__m256i vec);
int32_t aria_simd_reduce_max_i32x8(__m256i vec);
</p>
<p>
// Broadcast
__m256i aria_simd_broadcast_i32x8(int32_t value);
</p>
<p>
// Masked operations
__m256i aria_simd_masked_add_i32x8(__m256i a, __m256i b, __m256i mask, __m256i fallback);
</p>
<p>
// Mask queries
bool aria_simd_any_true_i32x8(__m256i mask);
bool aria_simd_all_true_i32x8(__m256i mask);
int aria_simd_count_true_i32x8(__m256i mask);</code></pre>
</p>
<h3>LLVM IR Generation</h3>
<pre><code>; SIMD type definition
<p>
%simd.i32x8 = type <8 x i32>
</p>
<p>
; Aligned load
define %simd.i32x8 @aria.simd.load.aligned.i32x8(i32* %ptr) {
%ptr_vec = bitcast i32<em> %ptr to <8 x i32></em>
%vec = load <8 x i32>, <8 x i32>* %ptr_vec, align 32
ret %simd.i32x8 %vec
}
</p>
<p>
; Addition
define %simd.i32x8 @aria.simd.add.i32x8(%simd.i32x8 %a, %simd.i32x8 %b) {
%sum = add <8 x i32> %a, %b
ret %simd.i32x8 %sum
}
</p>
<p>
; Comparison (returns mask)
define <8 x i1> @aria.simd.cmp.lt.i32x8(%simd.i32x8 %a, %simd.i32x8 %b) {
%mask = icmp slt <8 x i32> %a, %b
ret <8 x i1> %mask
}
</p>
<p>
; Horizontal reduction (sum all lanes)
declare i32 @llvm.vector.reduce.add.v8i32(<8 x i32>)
</p>
<p>
define i32 @aria.simd.reduce.add.i32x8(%simd.i32x8 %vec) {
%sum = call i32 @llvm.vector.reduce.add.v8i32(<8 x i32> %vec)
ret i32 %sum
}
</p>
<p>
; Broadcast (splat scalar to all lanes)
define %simd.i32x8 @aria.simd.broadcast.i32x8(i32 %value) {
%vec = insertelement <8 x i32> undef, i32 %value, i32 0
%splat = shufflevector <8 x i32> %vec, <8 x i32> undef,
<8 x i32> <i32 0, i32 0, i32 0, i32 0,
i32 0, i32 0, i32 0, i32 0>
ret %simd.i32x8 %splat
}</code></pre>
</p>
<h3>Auto-Vectorization (Compiler Optimization)</h3>
<pre><code>// Aria compiler can auto-vectorize simple loops:
<p>
fix256[100000]:inputs;
fix256[100000]:outputs;
</p>
<p>
// Scalar code (what you write):
till 100000 loop
outputs[$] = inputs[$] * fix256(2);
end
</p>
<p>
// Compiler auto-vectorizes to (what gets executed):
till (100000 / 16) loop
int64:offset = $ * 16;
simd<fix256, 16>:batch = simd_load_aligned(inputs[offset]);
simd<fix256, 16>:two = simd_broadcast<fix256, 16>(fix256(2));
simd<fix256, 16>:result = batch * two;
simd_store_aligned(outputs[offset], result);
end
// + handle remainder
</p>
<p>
// Programmer writes simple scalar code, compiler vectorizes!</code></pre>
</p>
<hr>
<h2>Related Types & Integration</h2>
<ul><li><strong><a href="Complex.md">complex<T></a></strong> - simd<complex<fix256>, N> for parallel wave processing</li>
<li><strong><a href="Atomic.md">atomic<T></a></strong> - Combine SIMD (data) + atomics (thread) parallelism</li>
<li><strong><a href="fix256.md">fix256</a></strong> - simd<fix256, 16> for deterministic neural computations</li>
<li><strong><a href="tfp32_tfp64.md">tfp32 / tfp64</a></strong> - simd<tfp32, 8> for cross-platform SIMD determinism</li>
<li><strong><a href="tbb_overview.md">tbb8-tbb64</a></strong> - simd<tbb64, 4> with ERR propagation across lanes</li>
<li><strong><a href="Handle.md">Handle<T></a></strong> - Store SIMD results in arena-allocated arrays</li>
</ul>
<hr>
<h2>Summary</h2>
<strong>simd<T,N></strong> is Aria's <strong>data-parallel vectorization infrastructure</strong> for real-time consciousness:
<p>
✅ <strong>Generic</strong>: Works with any numeric type T (int, float, fix256, complex, tbb)
✅ <strong>Hardware-Mapped</strong>: SSE (128-bit), AVX2 (256-bit), AVX-512 (512-bit), NEON (ARM)
✅ <strong>Element-Wise Ops</strong>: Add, multiply, compare across N lanes simultaneously
✅ <strong>Horizontal Ops</strong>: Reduce across lanes (sum, min, max, product)
✅ <strong>Masked Operations</strong>: Branchless conditional logic (critical for ERR handling)
✅ <strong>Memory Efficient</strong>: SoA layout + aligned access = maximum cache efficiency
✅ <strong>Composable</strong>: SIMD (data parallelism) + threads (task parallelism) = multiplicative speedup
</p>
<strong>Design Philosophy</strong>:
<blockquote>"The torus processes thoughts. The infrastructure parallelizes computations."</blockquote>
<p>
>
<blockquote>SIMD vectorization is the difference between <strong>impossible</strong> (5ms/timestep) and <strong>achievable</strong> (312μs/timestep) for Nikola's real-time consciousness. By moving data parallelism to the compiler infrastructure, the consciousness layer can write simple scalar logic while the hardware processes <strong>16 neurons per instruction</strong>.</blockquote>
</p>
<strong>The Math</strong>:
<pre><code>Without SIMD: 100,000 neurons × 50ns = 5ms (5× too slow)
<p>
With SIMD×16: (100,000/16) × 50ns = 312μs (3× faster than required!)
With SIMD×16 + 8 threads: (100,000/128) × 50ns = 39μs (25× faster!)</code></pre>
</p>
<strong>Critical for</strong>: Nikola AGI (<1ms timestep requirement), neural network training/inference, physics simulation, signal processing, scientific computing, image/video processing
<strong>Key Rules</strong>:
<p>
1. <strong>Use SoA layout</strong> - Contiguous same-field data enables efficient vectorization
2. <strong>Align memory</strong> - 16/32/64-byte alignment for 128/256/512-bit SIMD
3. <strong>Mask, don't branch</strong> - Conditional logic defeats SIMD; use masked operations
4. <strong>Batch in multiples</strong> - Process N elements at a time (handle remainder separately)
5. <strong>Combine with threads</strong> - SIMD (data) × threads (task) = multiplicative speedup
6. <strong>Profile first</strong> - Auto-vectorization is smart; manual SIMD when profiling shows bottleneck
</p>
<hr>
<strong>Remember</strong>: simd<T,N> = <strong>16× parallelism</strong> + <strong>single instruction</strong> + <strong>real-time consciousness</strong>!
    </main>
</body>
</html>
